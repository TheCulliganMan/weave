{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5214b543",
   "metadata": {},
   "source": [
    "- Load two eval_results\n",
    "\n",
    "EvalResult\n",
    "- example, label, result, item_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import weave\n",
    "import random\n",
    "import string\n",
    "from weave import weave_internal\n",
    "weave.use_frontend_devmode()\n",
    "from weave.panels import panel_board\n",
    "from weave import ops_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550daef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_string_n(n: int) -> str:\n",
    "    return \"\".join(\n",
    "        random.choice(string.ascii_uppercase + string.digits) for _ in range(n)\n",
    "    )\n",
    "\n",
    "dataset_raw = [{\n",
    "    'id': str(i),\n",
    "    'example': rand_string_n(10),\n",
    "    'label': random.choice(string.ascii_uppercase)} for i in range(50)]\n",
    "dataset = weave.save(dataset_raw, 'dataset')\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d930d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_row, config):\n",
    "    if random.random() < config['correct_chance']:\n",
    "        return dataset_row['label']\n",
    "    return random.choice(string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, predict_config):\n",
    "    eval_result = []\n",
    "    correct_count = 0\n",
    "    count = 0\n",
    "    for dataset_row in dataset:\n",
    "        result = predict(dataset_row, predict_config)\n",
    "        correct = dataset_row['label'] == result\n",
    "        if correct:\n",
    "            correct_count += 1\n",
    "        count +=1 \n",
    "        eval_result.append({\n",
    "            'dataset_id': dataset_row['id'],\n",
    "            'result': result,\n",
    "            'summary': correct\n",
    "        })\n",
    "    return {\n",
    "        'config': predict_config,\n",
    "        'eval_table': eval_result,\n",
    "        'summary': {'accuracy': correct_count / len(dataset)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d16a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_raw0 = evaluate(dataset_raw, {'correct_chance': 0.5})\n",
    "eval_result_raw1 = evaluate(dataset_raw, {'correct_chance': 0.5})\n",
    "eval_result0 = weave.save(eval_result_raw0, 'eval_result0')\n",
    "eval_result1 = weave.save(eval_result_raw1, 'eval_result1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8065ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#   - works on 2, make work on N\n",
    "#   - default table and panel behavior is horrible\n",
    "#   - automatically make good columns\n",
    "#   - make an example for text extraction (how is that usually evaluated?)\n",
    "#   - colors all the way through\n",
    "#   - controls?\n",
    "#   - hard examples, confusing examples\n",
    "#   - auto update when inputs change?\n",
    "#   - or do you just publish a new board when that happens?\n",
    "#   - maybe we want like an overview tab where we select a set of runs\n",
    "#     with latency v accuracy or whatever\n",
    "#     - and then an example drilldown tab where you just pick two and compare\n",
    "#   - going to want to compare traces for the LLM extraction case\n",
    "\n",
    "\n",
    "varbar = panel_board.varbar()\n",
    "\n",
    "dataset_var = varbar.add('dataset', dataset)\n",
    "eval_result0_var = varbar.add('eval_result0', eval_result0)\n",
    "eval_result1_var = varbar.add('eval_result1', eval_result1)\n",
    "\n",
    "summary = varbar.add('summary', weave.ops.make_list(\n",
    "    a=weave.ops.TypedDict.merge(weave.ops.dict_(name='res0'), eval_result0_var['summary']),\n",
    "    b=weave.ops.TypedDict.merge(weave.ops.dict_(name='res1'), eval_result1_var['summary']),\n",
    "))\n",
    "\n",
    "weave.ops.make_list(a=eval_result0_var['eval_table'], b=eval_result0_var['eval_table'])\n",
    "\n",
    "# join evals together first\n",
    "joined_evals = varbar.add('joined_evals', weave.ops.join_all(\n",
    "    weave.ops.make_list(a=eval_result0_var['eval_table'], b=eval_result1_var['eval_table']),\n",
    "    lambda row: row['dataset_id'],\n",
    "    False))\n",
    "\n",
    "# then join dataset to evals\n",
    "dataset_evals = varbar.add('dataset_evals', weave.ops.join_2(\n",
    "    dataset_var,\n",
    "    joined_evals,\n",
    "    lambda row: row['id'],\n",
    "    lambda row: row['dataset_id'][0],\n",
    "    'dataset',\n",
    "    'evals',\n",
    "    False,\n",
    "    False\n",
    "))\n",
    "\n",
    "\n",
    "main = weave.panels.Group(\n",
    "        layoutMode=\"grid\",\n",
    "        showExpressions=True,\n",
    "        enableAddPanel=True,\n",
    "    )\n",
    "\n",
    "#### Run/config info TODO\n",
    "\n",
    "#### Summary info\n",
    "\n",
    "main.add(\"accuracy\",\n",
    "         weave.panels.Plot(summary, x=lambda row: row['accuracy'], y=lambda row: row['name']),\n",
    "         layout=weave.panels.GroupPanelLayout(x=0, y=0, w=12, h=4))\n",
    "# main.add('dataset_table', dataset)\n",
    "# main.add('joined_evals', joined_evals)\n",
    "# main.add('dataset_evals', dataset_evals, layout=weave.panels.GroupPanelLayout(x=0, y=4, w=24, h=6))\n",
    "\n",
    "##### Example details\n",
    "\n",
    "# more ideas: show examples that all got wrong, or that are confusing\n",
    "\n",
    "eval_table = weave.panels.Table(dataset_evals)\n",
    "eval_table.config.rowSize = 2\n",
    "eval_table.add_column(lambda row: row['dataset.id'], 'id')\n",
    "eval_table.add_column(lambda row: row['dataset.example'], 'example')\n",
    "eval_table.add_column(lambda row: row['dataset.label'], 'label')\n",
    "eval_table.add_column(lambda row: weave.ops.dict_(res0=row['evals.result'][0], res1=row['evals.result'][1]), 'result')\n",
    "eval_table.add_column(lambda row: weave.ops.dict_(res0=row['evals.summary'][0], res1=row['evals.summary'][1]), 'summary')\n",
    "\n",
    "main.add('eval_table', eval_table, layout=weave.panels.GroupPanelLayout(x=0, y=4, w=24, h=12))\n",
    "\n",
    "weave.panels.Board(vars=varbar, panels=main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
